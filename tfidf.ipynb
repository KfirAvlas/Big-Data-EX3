{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL Java\"),\n",
    "    (2,\"SQL Java\")],\n",
    " [\"document\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+\n",
      "|document|sentence                 |\n",
      "+--------+-------------------------+\n",
      "|0       |Python python Spark Spark|\n",
      "|1       |Python SQL Java          |\n",
      "|2       |SQL Java                 |\n",
      "+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Estimator, transformers and evaluators\n",
    "Estimator produce transformaers by fitting on a dataset (for example: linear regression algorithm produces a linear regression model with fitted weights and an intercept, which is a transformer. The main method for using estimator is fit.\n",
    "Evaluators evaluate the performance of a model bases on a single metric (for example RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual \n",
    "terms (usually words). A simple Tokenizer class provides this functionality. \n",
    "The example below shows how to split sentences into sequences of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+\n",
      "|document|sentence                 |words                         |\n",
      "+--------+-------------------------+------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|\n",
      "|1       |Python SQL Java          |[python, sql, java]           |\n",
      "|2       |SQL Java                 |[sql, java]                   |\n",
      "+--------+-------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer  = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |\n",
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(4,[0,2],[2.0,2.0])      |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(4,[0,1,3],[1.0,1.0,1.0])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(4,[1,3],[1.0,1.0])      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\",  vocabSize=4,minDF=1.0)\n",
    "\n",
    "model = cv.fit(wordsData)\n",
    "\n",
    "result = model.transform(wordsData)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize=4, minDF=1.0)\n",
    "\n",
    "model = cv.fit(wordsData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |\n",
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(4,[0,2],[2.0,2.0])      |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(4,[0,1,3],[1.0,1.0,1.0])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(4,[1,3],[1.0,1.0])      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(wordsData)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF: IDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each feature. Intuitively, it down-weights features which appear frequently in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "\n",
    "#rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |features                                                                 |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(4,[0,2],[2.0,2.0])      |(4,[0,2],[0.5753641449035617,1.3862943611198906])                        |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(4,[0,1,3],[1.0,1.0,1.0])|(4,[0,1,3],[0.28768207245178085,0.28768207245178085,0.28768207245178085])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(4,[1,3],[1.0,1.0])      |(4,[1,3],[0.28768207245178085,0.28768207245178085])                      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_879c014ad63b, CountVectorizer_e7e0055b945c, IDF_04fbe42238c4]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"binary: Binary toggle to control the output vector values. If True, all nonzero counts (after minTF filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False (default: False)\\ninputCol: input column name. (current: words)\\nmaxDF: Specifies the maximum number of different documents a term could appear in to be included in the vocabulary. A term that appears more than the threshold will be ignored. If this is an integer >= 1, this specifies the maximum number of documents the term could appear in; if this is a double in [0,1), then this specifies the maximum fraction of documents the term could appear in. Default (2^63) - 1 (default: 9.223372036854776e+18)\\nminDF: Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer >= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default 1.0 (default: 1.0)\\nminTF: Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting. Default 1.0 (default: 1.0)\\noutputCol: output column name. (default: CountVectorizer_e7e0055b945c__output, current: rawFeatures)\\nvocabSize: max size of the vocabulary. Default 1 << 18. (default: 262144)\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[1].explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |features                                                                 |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(5,[0,4],[2.0,2.0])      |(5,[0,4],[1.3862943611198906,0.5753641449035617])                        |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(5,[1,2,4],[1.0,1.0,1.0])|(5,[1,2,4],[0.28768207245178085,0.28768207245178085,0.28768207245178085])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(5,[1,2],[1.0,1.0])      |(5,[1,2],[0.28768207245178085,0.28768207245178085])                      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(4, {0: 2.0, 3: 2.0}),\n",
       " SparseVector(4, {0: 1.0, 1: 1.0, 2: 1.0}),\n",
       " SparseVector(4, {1: 1.0, 2: 1.0})]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(sentenceData).select('rawFeatures').rdd.map(lambda row: row['rawFeatures']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_counts = model.transform(sentenceData)\\\n",
    "                    .select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "vocabList = model.stages[1].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'java', 'sql', 'spark']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|vocabList|counts|\n",
      "+---------+------+\n",
      "|   python|   3.0|\n",
      "|     java|   2.0|\n",
      "|      sql|   2.0|\n",
      "|    spark|   2.0|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(4, {0: 2.0, 3: 2.0})),\n",
       " Row(rawFeatures=SparseVector(4, {0: 1.0, 1: 1.0, 2: 1.0})),\n",
       " Row(rawFeatures=SparseVector(4, {1: 1.0, 2: 1.0}))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = model.transform(sentenceData).select('rawFeatures').collect()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |features                                                                 |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(4,[0,3],[2.0,2.0])      |(4,[0,3],[0.5753641449035617,1.3862943611198906])                        |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(4,[0,1,2],[1.0,1.0,1.0])|(4,[0,1,2],[0.28768207245178085,0.28768207245178085,0.28768207245178085])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(4,[1,2],[1.0,1.0])      |(4,[1,2],[0.28768207245178085,0.28768207245178085])                      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingTF\n",
    "HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a “set of terms” might be a bag of words. HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3. Then term frequencies are calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures              |features                                                                 |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(5,[0,4],[2.0,2.0])      |(5,[0,4],[1.3862943611198906,0.5753641449035617])                        |\n",
      "|1       |Python SQL Java          |[python, sql, java]           |(5,[1,2,4],[1.0,1.0,1.0])|(5,[1,2,4],[0.28768207245178085,0.28768207245178085,0.28768207245178085])|\n",
      "|2       |SQL Java                 |[sql, java]                   |(5,[1,2],[1.0,1.0])      |(5,[1,2],[0.28768207245178085,0.28768207245178085])                      |\n",
      "+--------+-------------------------+------------------------------+-------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL Java\"),\n",
    "    (2,\"SQL Java\")],\n",
    " [\"document\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "\n",
    "model = pipeline.fit(sentenceData)\n",
    "model.transform(sentenceData).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
